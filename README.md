ğŸŒ• Lunar Lander Implementation using Deep Q-Network (DQN)

This project implements a Deep Q-Network (DQN) agent to solve the OpenAI Gym / Gymnasium LunarLander environment.
The agent learns optimal landing behavior using experience replay, target networks, and epsilon-greedy exploration, achieving stable learning in a continuous state space with discrete actions.

If the LunarLander environment is unavailable, the code automatically falls back to CartPole-v1.

ğŸš€ Features

âœ… Deep Q-Network (DQN) using PyTorch

ğŸ§  Experience Replay Buffer

ğŸ¯ Target Network for stable learning

ğŸ”„ Epsilon-Greedy Exploration with decay

ğŸ“ˆ Training score visualization

ğŸ® Real-time environment rendering


| Technique            | Purpose                             |
| -------------------- | ----------------------------------- |
| Experience Replay    | Breaks correlation between samples  |
| Target Network       | Stabilizes Q-value updates          |
| Epsilon Decay        | Balances exploration & exploitation |
| Fixed Target Updates | Prevents oscillations               |





âš™ï¸ Environment Details
LunarLander

State Space: 8 continuous variables

Action Space: 4 discrete actions

Solved Threshold: â‰¥ 200 average reward over 100 episodes




Actions:
0 â†’ Do nothing
1 â†’ Fire left orientation engine
2 â†’ Fire main engine
3 â†’ Fire right orientation engine



State Variables:
[ x_position,
  y_position,
  x_velocity,
  y_velocity,
  angle,
  angular_velocity,
  left_leg_contact,
  right_leg_contact ]



ğŸ§ª Training Process

Replay buffer size: 10,000

Batch size: 64

Target update frequency: 100 steps

Initial epsilon: 1.0

Minimum epsilon: 0.01

Epsilon decay: 0.995

Episodes: 1000

The environment is considered solved once the average reward over 100 episodes exceeds the threshold.

ğŸ“Š Visualization

The training process includes:

ğŸ“ˆ Episode reward curve

ğŸ“‰ Moving average (100 episodes)

ğŸ“Š Score distribution histogram

ğŸ® Optional real-time rendering

â–¶ï¸ How to Run
1ï¸âƒ£ Install Dependencies
pip install gymnasium[box2d] torch numpy matplotlib


2ï¸âƒ£ Run Training
python main.py


The script will:

Train the agent

Render the environment (optional)

Plot training statistics

Test the trained agent


ğŸ§ª Testing the Agent

After training, the agent is evaluated over multiple test episodes using a greedy policy (no exploration).

Metrics reported:

Average score

Maximum & minimum score

Standard deviation

ğŸ’¾ Model Saving

The trained DQN model is saved using:

torch.save(agent.q_network.state_dict(), 'lunar_lander_dqn.pth')

ğŸ§  Key Learning Outcomes

Understanding DQN architecture and training

Handling continuous state spaces with neural networks

Stabilizing reinforcement learning using target networks

Practical experience with Gymnasium environments

RL training visualization and evaluation

ğŸ”® Future Improvements

Double DQN

Dueling DQN Architecture

Prioritized Experience Replay

Hyperparameter tuning

Training speed optimization with vectorized environments

ğŸ“Œ Technologies Used

Python

PyTorch

Gymnasium (OpenAI Gym)

NumPy

Matplotlib

ğŸ‘¤ Author:
Hitesh Kumar
AI / ML Enthusiast | Reinforcement Learning Practitioner
